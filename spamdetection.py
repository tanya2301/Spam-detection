# -*- coding: utf-8 -*-
"""spamdetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1My0r791o1pGBxNg9_jZ08Goli06LoZK0
"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/spam.csv',encoding='Latin-1')
df.info()

df=df.drop(columns=['Unnamed: 2' ,'Unnamed: 3' ,'Unnamed: 4' ])

df=df.rename(columns={'v1':'target','v2':'text'})

from sklearn.preprocessing import LabelEncoder
encoder= LabelEncoder()
df['target']=encoder.fit_transform(df.target)

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(keep='first',inplace=True)

df.duplicated().sum()

df.shape

import matplotlib.pyplot as plt

df['target'].value_counts()

plt.pie(df['target'].value_counts(),labels=['ham','spam'],autopct='%0.2f')
plt.show()

import nltk  #for natural language processing
nltk.download('punkt')  #punkt is a package ; nltk dependencies

df['no_char'] = df.text.apply(len) #for count the lenghth of character in df

df['no_word'] = df.text.apply(lambda x:len(nltk.word_tokenize(x))) 
#lambda is anonymous function in which we can pass n no. of arguments
#tokenize is used for count

df['no_sent'] = df.text.apply(lambda x:len(nltk.sent_tokenize(x))) #for count the sentence

df[['no_char', 'no_word','no_sent']].describe #for describing above info
df

#analysis of ham msgs
df[df['target']==0][['no_char', 'no_word','no_sent']].describe()

#analysis of spam msgs
df[df['target']==1][['no_char', 'no_word','no_sent']].describe()

plt.figure(figsize=(15,5)) 
import seaborn as sns #another library for plotting graph
sns.histplot(df[df['target']==0]['no_char']) #ham 
sns.histplot(df[df['target']==1]['no_char'],color='red')  #spam
#x axis --> independent variable
#y axis-->  dependent variable

plt.figure(figsize=(15,5))
import seaborn as sns #another library for plotting graph
sns.histplot(df[df['target']==0]['no_word']) #ham 
sns.histplot(df[df['target']==1]['no_word'],color='red')  #spam

plt.figure(figsize=(15,5))
import seaborn as sns #another library for plotting graph
sns.histplot(df[df['target']==0]['no_sent']) #ham 
sns.histplot(df[df['target']==1]['no_sent'],color='red')  #spam

sns.pairplot(df,hue='target') 
#for plotting the graph which shows correaltion of features with each other

#alternative method for correlation graph -->  HEATMAP
sns.heatmap(df.corr(),annot=True) #annot for numbered labelling in map
# if the value is 1 then it is highly correalted
# value is less than 1 then , not much correalted

#Text processing
# 1. lower case
# 2. tokenize
# 3. remove special charecter
# 4. remove stop words and punctution
# 5. stemming

text = 'i am #TANYA rawat living in @indore'

def text_transformed(text):
  text=text.lower()
  text=nltk.word_tokenize(text)
  y=[]
  for i in text:
    if i.isalnum():   #alnum=alphanumeric
      y.append(i)
    text=y[:]
  y.clear()  
  for i in text:
    if i not in k and  i not in string.punctuation:
       y.append(i)
    text=y[:]
  y.clear()
  for i in text:
    y.append(ps.stem(i))  
  text=y[:]  
  return ' '.join(y)

df['text_transformed']=df['text'].apply(text_transformed)

df.head()

nltk.download('stopwords')

from nltk.corpus import stopwords #corpus means set of words
k= stopwords.words('english')
k

import string
string.punctuation

from nltk.stem.porter import PorterStemmer
ps=PorterStemmer()
ps.stem('pricing')

from wordcloud import WordCloud #for making word cloud of most used words

wc= WordCloud(width=500,height=500,min_font_size=10,background_color='white')
wc_ham=wc.generate(df[df['target']==0]['text_transformed'].str.cat(sep=""))

plt.figure(figsize=(10,10))
plt.imshow(wc_ham)

wc= WordCloud(width=500,height=500,min_font_size=10,background_color='white')
wc_spam=wc.generate(df[df['target']==1]['text_transformed'].str.cat(sep=""))

plt.figure(figsize=(10,10))
plt.imshow(wc_spam)

from collections import Counter #for printing top most words

spam_corpus=[]
for msg in df[df['target']==1]['text_transformed'].to_list():
  for word in msg.split():
    spam_corpus.append(word)
print(spam_corpus)

ham_corpus=[]
for msg in df[df['target']==0]['text_transformed'].to_list():
  for word in msg.split():
    ham_corpus.append(word)
print(ham_corpus)

from collections import Counter

sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.show()

sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])
plt.xticks(rotation='vertical')
plt.show()

#model building
from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer()
X=cv.fit_transform(df['text_transformed']).toarray()
X
X.shape
y=df.target.values
y

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score  #another method for checking accuracy

X_train,X_test, y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
gnb=GaussianNB()
gnb.fit(X_train,y_train)
ypred=gnb.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

bnb=BernoulliNB()
bnb.fit(X_train,y_train)
ypred=bnb.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

mnb=MultinomialNB()
mnb.fit(X_train,y_train)
ypred=mnb.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier,ExtraTreesClassifier,RandomForestClassifier
from xgboost import XGBClassifier

svc = SVC(kernel='sigmoid', gamma=1.0) 
knc = KNeighborsClassifier()
dtc = DecisionTreeClassifier(max_depth=5) 
lrc = LogisticRegression (solver='liblinear', penalty='11') 
rfc = RandomForestClassifier(n_estimators=50, random_state=2) 
abc = AdaBoostClassifier (n_estimators=50, random_state=2) 
bc = BaggingClassifier (n_estimators=50, random_state=2) 
etc = ExtraTreesClassifier (n_estimators=50, random_state=2) 
godt = GradientBoostingClassifier (n_estimators=50, random_state=2) 
xgb = XGBClassifier (n_estimators=50, random_state=2)

#svc
svc.fit(X_train,y_train)
ypred=svc.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

#KNeighborsClassifier
knc.fit(X_train,y_train)
ypred=knc.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

#DecisionTreeClassifier
dtc.fit(X_train,y_train)
ypred=dtc.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

#logistic
lrc.fit(X_train,y_train)
ypred=lrc.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

#RandomForestClassifier
rfc.fit(X_train,y_train)
ypred=rfc.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

#AdaBoostClassifier 
abc.fit(X_train,y_train)
ypred=abc.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

#BaggingClassifier 
bc.fit(X_train,y_train)
ypred=bc.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

#ExtraTreesClassifier 
etc.fit(X_train,y_train)
ypred=etc.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

#GradientBoostingClassifier 
godt.fit(X_train,y_train)
ypred=godt.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))

#XGBClassifier
xgb.fit(X_train,y_train)
ypred=xgb.predict(X_test)
print(accuracy_score(y_test,ypred))
print(confusion_matrix(y_test,ypred))
print(precision_score(y_test,ypred))